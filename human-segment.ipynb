{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae9985e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342cf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW,SGD\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6528c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self,images_dir,mask_dir):\n",
    "        self.image_dir=images_dir\n",
    "        self.mask_dir=mask_dir\n",
    "        self.transform=transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "        )\n",
    "        self.valid_extension={\".jpg\",\".jpeg\",\".png\"}\n",
    "        self.images=[f for f in os.listdir(self.image_dir) if os.path.splitext(f)[1].lower() in self.valid_extension]\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, index):\n",
    "        image_path= os.path.join(self.image_dir,self.images[index])\n",
    "        name,ext=os.path.splitext(self.images[index])\n",
    "        masked_path=os.path.join(self.mask_dir,f\"{name}.png\")\n",
    "        image=Image.open(image_path).convert(\"RGB\")\n",
    "        mask=Image.open(masked_path).convert(\"L\")\n",
    "        image=self.transform(image)\n",
    "        mask=self.transform(mask)\n",
    "        mask=(mask>0.5).float()\n",
    "        return image,mask\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48cbb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(image_dir,mask_dir,batch_size=2,shuffle=True):\n",
    "    dataset=SegmentationDataset(images_dir=image_dir,\n",
    "                        mask_dir=mask_dir)\n",
    "    return DataLoader(dataset,batch_size=batch_size,shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5ed255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d45c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "        return down, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adabc76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # handle size mismatch (odd dimensions)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08aedbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.downconv_1 = DownSample(in_channels, 64)\n",
    "        self.downconv_2 = DownSample(64, 128)\n",
    "        self.downconv_3 = DownSample(128, 256)\n",
    "        self.downconv_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up_conv1 = UpSample(1024, 512)\n",
    "        self.up_conv2 = UpSample(512, 256)\n",
    "        self.up_conv3 = UpSample(256, 128)\n",
    "        self.up_conv4 = UpSample(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_1, p1 = self.downconv_1(x)\n",
    "        down_2, p2 = self.downconv_2(p1)\n",
    "        down_3, p3 = self.downconv_3(p2)\n",
    "        down_4, p4 = self.downconv_4(p3)\n",
    "\n",
    "        b = self.bottleneck(p4)\n",
    "\n",
    "        up_1 = self.up_conv1(b, down_4)\n",
    "        up_2 = self.up_conv2(up_1, down_3)\n",
    "        up_3 = self.up_conv3(up_2, down_2)\n",
    "        up_4 = self.up_conv4(up_3, down_1)\n",
    "\n",
    "        out = self.out(up_4)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae18e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Apply sigmoid to logits\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_score = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "\n",
    "        # Return 1 - Dice Coefficient → smaller = better\n",
    "        return 1 - dice_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e3539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(BCEWithDiceLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss(smooth)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        dice_loss = self.dice(inputs, targets)\n",
    "        return 0.5 * bce_loss + dice_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6e40915",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "\n",
    "def train(model, dataloader, epochs=2, lr=0.001, save_path=\"unet_model\", load_path=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if load_path and os.path.exists(load_path):\n",
    "        print(f\"Loading model weights from {load_path}\")\n",
    "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    else:\n",
    "        print(f\"No checkpoint found, training from scratch\")\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = BCEWithDiceLoss()  # use your combined loss\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, masks in dataloader:\n",
    "            # ✅ move data to same device as model\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, LR: {lr}\")\n",
    "\n",
    "        # ✅ Save every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == epochs:\n",
    "            torch.save(model.state_dict(), f\"{save_path}_{epoch+1}.pth\")\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    print(f\"Training complete! Final model saved to {save_path}_final.pth\")\n",
    "    torch.save(model.state_dict(), f\"{save_path}_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7747156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=get_dataloader(\"cnn/data/Human-Segmentation-Dataset/Training_Images\",\"cnn/data/Human-Segmentation-Dataset/Ground_Truth\",batch_size=8,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "180e3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Unet(3,num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745f43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.0+cu130 13.0\n",
      "Torchvision: 0.24.0+cu130\n",
      "Torchaudio: 2.9.0+cu130\n",
      "GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision, torchaudio\n",
    "print(\"Torch:\", torch.__version__, torch.version.cuda)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"Torchaudio:\", torchaudio.__version__)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7363fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch\n",
      "Using device: cuda\n",
      "Epoch [1/2], Loss: 0.9845, LR: 0.001\n",
      "Epoch [2/2], Loss: 0.9870, LR: 0.001\n",
      "Checkpoint saved at epoch 2\n",
      "Training complete! Final model saved to unet_model_final.pth\n"
     ]
    }
   ],
   "source": [
    "train(model,dataloader,2,lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
